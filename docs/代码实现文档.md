# transSATSolver 代码实现文档

## 1. 代码架构概述

### 1.1 项目结构

transSATSolver 是一个使用 Transformer 模型（GPT-2/LLaMA）来解决 SAT（布尔可满足性）问题的深度学习系统。项目代码采用模块化设计，主要分为以下几个核心模块：

```
src/
├── core/           # 训练和评估核心模块
│   ├── train.py    # 主训练脚本
│   ├── trainer.py  # 自定义HuggingFace训练器
│   └── evaluator.py # 模型评估器
├── dataset/        # 数据处理模块
│   └── sat_dataset.py # SAT数据集和自定义分词器
├── models/         # 模型实现
│   ├── parat/      # PARAT模型实现
│   └── sat_solver/ # SAT求解器实现
└── utils/          # 工具函数
    ├── configurator.py # 配置管理
    └── utils.py        # 通用工具函数
```

### 1.2 模块职责与关系

#### 核心训练模块 (src/core/)
- **train.py**: 整个训练流程的入口点，负责模型初始化、数据加载、训练执行
- **trainer.py**: 继承HuggingFace Trainer，实现自定义优化器和学习率调度器
- **evaluator.py**: 批量生成和评估模型在SAT/UNSAT分类任务上的性能

#### 数据处理模块 (src/dataset/)
- **sat_dataset.py**: 处理SAT问题的文本表示，包括自定义分词器和数据集类

#### 模型实现 (src/models/)
- **parat/**: 包含自定义Transformer架构，支持GLU激活和多头自注意力
- **sat_solver/**: DPLL和CDCL算法实现，用于生成训练数据的执行轨迹

#### 工具模块 (src/utils/)
- **configurator.py**: nanoGPT风格的配置系统，支持Python配置文件
- **utils.py**: 模型加载、停止条件、数据路径管理等工具函数

## 2. 核心算法实现

### 2.1 训练流程 (train.py)

训练脚本的核心逻辑：

```python
# 1. 配置加载和参数设置
exec(open('src/utils/configurator.py').read())

# 2. 自定义分词器初始化
if state_trace:
    # 包含求解器状态跟踪的token集
    custom_tokens = [str(i) for i in range(30 + 1)] + 
                   [str(-i) for i in range(1, 30 + 1)] + 
                   ["[SEP]", "SAT", "UNSAT", "[EOS]", "Decide", "[UP]", "D", "[BT]", "[UNK]"]
else:
    # 基础token集
    custom_tokens = [str(i) for i in range(30 + 1)] + 
                   [str(-i) for i in range(1, 30 + 1)] + 
                   ["[SEP]", "SAT", "UNSAT", "[EOS]", "[UNK]", "(", ")"]

tokenizer = CustomTokenizer(custom_tokens)

# 3. 模型初始化（GPT-2或LLaMA）
if model == "gpt2":
    config = GPT2Config(
        vocab_size=len(tokenizer.vocab),
        n_ctx=block_size,
        n_embd=n_embd,
        n_layer=n_layer,
        n_head=n_head
    )
    model = GPT2LMHeadModel(config)
elif model == "llama":
    config = LlamaConfig(
        vocab_size=len(tokenizer.vocab),
        max_position_embeddings=block_size,
        num_hidden_layers=n_layer,
        num_attention_heads=n_head,
        hidden_size=n_embd
    )
    model = LlamaForCausalLM(config)

# 4. 数据集加载和预处理
dataset = SATDataset(
    file_path=dataset_path,
    tokenizer=tokenizer,
    max_id=max_id,
    block_size=block_size,
    shift_within_block=rand_pos,  # 随机位置偏移
    permute_constants=perm_vars,   # 变量重排列
    mask_formula=mask_formula      # 掩码CNF公式部分
)

# 5. 训练执行
trainer = SATHFTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)
trainer.train()
```

### 2.2 自定义训练器 (trainer.py)

SATHFTrainer 继承自 HuggingFace Trainer，自定义了以下关键组件：

```python
class SATHFTrainer(Trainer):
    
    def create_optimizer(self):
        # 使用AdamW优化器，配置专门针对SAT任务的超参数
        self.optimizer = AdamW(
            self.model.parameters(), 
            lr=6e-4,                    # 学习率
            betas=(0.9, 0.95),         # Adam的beta参数
            weight_decay=0.1           # 权重衰减
        )
        return self.optimizer
    
    def create_scheduler(self, num_training_steps: int, optimizer=None):
        # 余弦退火学习率调度器，带2000步预热
        self.lr_scheduler = get_cosine_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=2000, 
            num_training_steps=num_training_steps
        )
        return self.lr_scheduler
    
    def training_step(self, model, inputs):
        # 训练步骤，添加梯度裁剪防止梯度爆炸
        outputs = super().training_step(model, inputs)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        return outputs
```

### 2.3 DPLL求解器实现 (dpll.py)

DPLL算法是SAT求解的基础算法，项目实现了带执行轨迹记录的DPLL：

```python
def dpll(clauses, n_vars, heuristic, assignment, tracer=None, MAX_ITER=None, polarity=True):
    # 初始化轨迹记录器
    if tracer is None:
        tracer = AssignTrace()
    tracer.dpll_start()
    
    # 选择未赋值变量
    rem_vars = [x for x in range(1, n_vars + 1) if assignment[x] is None]
    var = heuristic(clauses, rem_vars)
    
    # 尝试正向赋值
    assignment[abs(var)] = (var > 0)
    tracer.active_assign(assignment, var)
    new_clauses = update_formula(clauses, var)
    
    if new_clauses == 'UNSAT':
        tracer.unsat()
    else:
        # 单元子句传播（BCP）
        res, new_clauses, new_assignment = bcp(new_clauses, assignment, tracer, polarity=polarity)
        if res == 'SAT':
            return 'SAT', new_assignment
        if res == 'UNSAT':
            tracer.unsat()
        else:
            # 递归调用
            res, new_assignment = dpll(new_clauses, n_vars, heuristic, new_assignment, tracer, MAX_ITER, polarity=polarity)
            if res == 'SAT':
                return 'SAT', new_assignment
    
    # 回溯并尝试反向赋值
    tracer.unassign(assignment, var)
    # ... 类似逻辑处理反向赋值
```

## 3. 数据流程

### 3.1 数据格式

SAT问题以文本格式存储，格式如下：
```
[CNF公式] [SEP] [执行轨迹] [SAT/UNSAT]
```

示例：
```
1 -2 0 3 -1 0 2 -3 0 [SEP] 1 -2 3 UNSAT
```

### 3.2 数据预处理流程

SATDataset类实现了复杂的数据预处理：

```python
class SATDataset(Dataset):
    def __getitem__(self, i):
        line = self.examples[i]
        
        # 1. 变量重排列（数据增强）
        if self.permute_constants:
            line = self.constant_permuter(line)
        
        # 2. 分词和填充
        tokens = self.tokenizer(
            line, 
            truncation=True, 
            padding="max_length", 
            max_length=self.block_size, 
            return_tensors="pt"
        )
        
        # 3. 创建标签（用于语言模型训练）
        labels = input_ids.clone()
        labels[labels == pad_token_id] = -100  # 忽略填充token
        
        # 4. 掩码CNF公式部分（可选）
        if self.mask_formula:
            sep_token_id = self.tokenizer.encode("[SEP]")[0]
            labels[:torch.nonzero(input_ids == sep_token_id)[0][0]] = -100
        
        # 5. 随机位置偏移（数据增强）
        if self.shift_within_block:
            size_left_pad = torch.randint(high=(block_size - input_len), size=(1,)).item()
            input_ids = self.left_pad(input_ids, size_left_pad, pad_token_id, block_size)
        
        return {"input_ids": input_ids, "labels": labels, "attention_mask": attention_mask}
```

### 3.3 自定义分词器

CustomTokenizer专门为SAT问题设计：

```python
class CustomTokenizer(PreTrainedTokenizer):
    def __init__(self, vocab_list: list):
        # 构建词汇表：数字、负号、特殊标记
        self.vocab = {v: k for k, v in enumerate(vocab_list)}
        self.ids_to_tokens = {k: v for v, k in self.vocab.items()}
        
    def tokenize(self, text):
        # 简单的空格分割
        return text.split()
    
    def save_vocabulary(self, save_directory):
        # 保存词汇表到文件
        vocab_file = os.path.join(save_directory, "vocab.txt")
        with open(vocab_file, "w") as writer:
            for token in self.vocab.keys():
                writer.write(token + "\n")
```

## 4. 模型架构

### 4.1 PARAT模型架构

PARAT模块实现了增强的Transformer架构：

#### 多头自注意力 (MultiHeadSelfAttention)
```python
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, hidden_size, num_heads, attn_bias=False):
        self.q_proj = nn.Linear(embed_dim, hidden_size, bias=attn_bias)
        self.k_proj = nn.Linear(embed_dim, hidden_size, bias=attn_bias)
        self.v_proj = nn.Linear(embed_dim, hidden_size, bias=attn_bias)
        self.out_proj = nn.Linear(hidden_size, embed_dim, bias=attn_bias)
    
    def forward(self, x, mask=None):
        # 投影到Q、K、V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # 多头重塑
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # 缩放点积注意力
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        
        # 应用因果掩码
        if mask is not None:
            attn_weights = attn_weights.masked_fill(mask == 0, -1e4)
        
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        return self.out_proj(attn_output)
```

#### GLU激活的MLP层
```python
class GLU(nn.Module):
    def __init__(self, activation='relu'):
        super(GLU, self).__init__()
        self.activation = activation
    
    def forward(self, x):
        x, gate = x.chunk(2, dim=-1)  # 分割为值和门控
        return x * act_map[self.activation](gate)  # 门控激活

class MLPLayer(nn.Module):
    def __init__(self, embed_dim, mlp_dim, activation='relu'):
        self.linear1 = nn.Linear(embed_dim, mlp_dim * 2)  # 2倍维度用于GLU
        self.glu = GLU(activation=activation)
        self.linear2 = nn.Linear(mlp_dim, embed_dim)
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.glu(x)  # 门控线性单元
        x = self.linear2(x)
        return x
```

### 4.2 模型配置

支持两种主要架构：

#### GPT-2配置
```python
config = GPT2Config(
    vocab_size=len(tokenizer.vocab),  # 词汇表大小
    n_ctx=800,                        # 上下文长度
    n_embd=768,                       # 嵌入维度
    n_layer=12,                       # 层数
    n_head=12,                        # 注意力头数
    n_positions=800                   # 位置编码长度
)
```

#### LLaMA配置（支持RoPE位置编码）
```python
config = LlamaConfig(
    vocab_size=len(tokenizer.vocab),
    max_position_embeddings=800,
    num_hidden_layers=12,
    num_attention_heads=12,
    hidden_size=768,
    intermediate_size=3072  # 4 * hidden_size
)
```

## 5. 关键类和函数

### 5.1 核心类

#### SATDataset
- **职责**: 处理SAT问题数据，支持多种数据增强技术
- **关键方法**:
  - `__getitem__`: 返回模型输入的张量
  - `constant_permuter`: 变量重排列
  - `left_pad`: 随机位置填充

#### CustomTokenizer
- **职责**: SAT问题专用分词器
- **关键方法**:
  - `tokenize`: 文本分词
  - `save_vocabulary`: 保存词汇表
  - `from_pretrained`: 加载预训练词汇表

#### SATHFTrainer
- **职责**: 自定义训练器，优化SAT任务训练
- **关键方法**:
  - `create_optimizer`: 创建AdamW优化器
  - `create_scheduler`: 创建学习率调度器
  - `training_step`: 自定义训练步骤

### 5.2 工具函数

#### 停止条件 (SATStoppingCriteria)
```python
class SATStoppingCriteria(StoppingCriteria):
    def __init__(self, tokenizer, stop_tokens=['SAT', 'UNSAT', '[EOS]']):
        self.stops = [tokenizer.encode(token)[0] for token in stop_tokens]
    
    def __call__(self, input_ids, scores):
        # 检查是否所有序列都包含停止标记
        for row in input_ids:
            if not any(stop_id in row for stop_id in self.stops):
                return False
        return True
```

#### 模型加载
```python
def load_model_and_tokenizer(model_dir, padding_side="left"):
    model = AutoModelForCausalLM.from_pretrained(model_dir)
    tokenizer = CustomTokenizer.from_pretrained(model_dir)
    tokenizer.padding_side = padding_side
    return model, tokenizer
```

#### 数据集路径管理
```python
def get_dataset_path(dir, split='train', ext='txt'):
    raw_path = os.path.join(dir, f'{split}.{ext}')
    if not os.path.exists(raw_path):
        # 自动运行prepare.py准备数据
        os.chdir(dir)
        os.system(f'python prepare.py')
    return raw_path
```

## 6. 配置系统

### 6.1 配置文件格式

配置文件是纯Python文件，定义训练参数：

```python
# configs/train_sat_6_10_random_rope_state_large.py
epochs = 5
block_size = 800
batch_size = 16
out_dir = 'results/models/sat-6-10-random-rope-state-large'
dataset = "data/datasets/SAT_6_10_Random_State_Large"
old_tokenizer = False
state_trace = True      # 包含求解器状态轨迹
rand_pos = False       # 随机位置偏移
perm_vars = True       # 变量重排列
mask_formula = True    # 掩码公式部分
model = "llama"        # 使用LLaMA架构
```

### 6.2 配置加载机制

configurator.py实现了灵活的配置系统：

```python
# 支持两种配置方式：
# 1. 配置文件：python train.py configs/config.py
# 2. 命令行参数：python train.py --batch_size=32 --epochs=10

for arg in sys.argv[1:]:
    if '=' not in arg:
        # 配置文件
        config_file = arg
        exec(open(config_file).read())
    else:
        # 命令行参数覆盖
        key, val = arg.split('=')
        key = key[2:]  # 去掉 '--'
        globals()[key] = literal_eval(val)
```

### 6.3 关键配置参数

| 参数 | 说明 | 默认值 |
|-----|------|--------|
| epochs | 训练轮数 | 20 |
| batch_size | 批次大小 | 12 |
| block_size | 序列最大长度 | 800 |
| max_id | 最大变量ID | 30 |
| n_layer | Transformer层数 | 12 |
| n_embd | 嵌入维度 | 768 |
| n_head | 注意力头数 | 12 |
| model | 模型类型 | "gpt2" |
| state_trace | 是否包含状态轨迹 | False |
| mask_formula | 是否掩码公式 | True |
| perm_vars | 是否重排列变量 | True |
| rand_pos | 是否随机位置 | True |

## 7. 扩展指南

### 7.1 添加新的模型架构

1. 在`src/core/train.py`中添加新模型的初始化逻辑：

```python
elif model == "your_model":
    config = YourModelConfig(
        vocab_size=len(tokenizer.vocab),
        # ... 其他配置
    )
    model = YourModel(config)
```

2. 确保模型兼容HuggingFace的生成API

### 7.2 添加新的数据增强技术

在`SATDataset`类中添加新的预处理方法：

```python
class SATDataset(Dataset):
    def __init__(self, ..., new_augmentation=False):
        self.new_augmentation = new_augmentation
    
    def __getitem__(self, i):
        if self.new_augmentation:
            line = self.apply_new_augmentation(line)
        # ... 其他处理
```

### 7.3 自定义评估指标

在`evaluator.py`中添加新的评估指标：

```python
from sklearn.metrics import your_metric

# 在评估部分添加
your_score = your_metric(true_labels, pred_labels)
print(f"Your Metric: {your_score}")
```

### 7.4 扩展SAT求解器

1. 在`src/models/sat_solver/`中实现新的求解算法
2. 继承或修改`AssignTrace`类以记录新的轨迹信息
3. 更新tokenizer以支持新的轨迹标记

### 7.5 优化训练流程

修改`SATHFTrainer`类以实现自定义训练策略：

```python
class SATHFTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # 自定义损失计算
        outputs = model(**inputs)
        loss = custom_loss_function(outputs, inputs['labels'])
        return (loss, outputs) if return_outputs else loss
```

## 8. 性能优化建议

### 8.1 内存优化
- 使用梯度累积减少批次大小需求
- 启用混合精度训练 (fp16)
- 使用梯度检查点技术

### 8.2 训练速度优化
- 使用更高效的注意力实现（如Flash Attention）
- 优化数据加载管道
- 使用分布式训练

### 8.3 模型效果优化
- 调整学习率和调度策略
- 实验不同的数据增强组合
- 尝试不同的模型架构和大小

## 9. 调试和故障排除

### 9.1 常见问题

**问题1**: 内存溢出
- 解决方案：减小batch_size或block_size

**问题2**: 训练不收敛
- 解决方案：调整学习率，检查数据质量

**问题3**: 生成结果不包含SAT/UNSAT
- 解决方案：增加max_length，使用停止条件

### 9.2 调试工具

- 使用`debug=True`模式运行
- 检查tokenizer输出：`print(tokenizer.vocab)`
- 监控训练指标：启用wandb记录

## 10. 总结

transSATSolver项目实现了一个完整的端到端系统，将Transformer模型应用于SAT问题求解。系统的关键创新包括：

1. **专用数据表示**：将SAT问题和求解轨迹编码为序列
2. **自定义架构**：支持GPT-2和LLaMA，带GLU激活的增强Transformer
3. **数据增强**：变量重排列、随机位置偏移等技术
4. **灵活配置**：Python配置文件系统，易于实验管理
5. **完整工具链**：从数据生成到模型评估的完整流程

该系统为研究神经网络在逻辑推理任务上的能力提供了强大的实验平台。