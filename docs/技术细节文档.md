# transSATSolver 技术细节文档

## 1. 核心算法与方法

### 1.1 DPLL算法基础
项目基于Davis-Putnam-Logemann-Loveland (DPLL)算法，这是一个完整的回溯SAT求解过程。

**主要操作**：
- **决策(Decision)**：选择一个未赋值的变量，尝试性地设置为True或False
- **单元传播(Unit Propagation)**：重复添加从单元子句中强制的文字
- **冲突/回溯(Conflict/Backtrack)**：出现空子句时，按时间顺序撤销决策

**抽象DPLL状态转换系统**：
```
UnitPropagate: M ∥ F ∧ (C ∨ l) ⟹ M ∘ l ∥ F ∧ (C ∨ l)
Decide:        M ∥ F ⟹ M ∘ l^d ∥ F  
Backjump:      M ∘ l^d ∘ N ∥ F ⟹ M ∘ l' ∥ F
Fail:          M ∥ F ∧ C ⟹ UNSAT
Success:       M ∥ F ⟹ SAT if M ⊨ F
```

### 1.2 理论构造核心（定理4.5）

#### 关键技术创新
1. **并行子句处理**：通过向量运算同时处理所有子句
2. **二进制编码**：将子句和赋值编码为二进制向量
3. **注意力机制实现逻辑运算**：使用自注意力实现复杂的逻辑推理

#### 构造参数
- **层数L**：7层
- **注意力头数H**：5个
- **嵌入维度d_emb**：O(p)
- **总参数量**：O(p²)

### 1.3 关键引理

#### 引理4.7：逻辑运算的向量化表示
对于3-SAT公式F和部分赋值A：

1. **可满足性检查**：
   ```
   A ⊨ F ⟺ min_{i∈[c]} E(C_i) · E(A) ≥ 1
   ```

2. **冲突检测**：
   ```
   F ⊨ ¬A ⟺ min_{i∈[c]} E(C_i) · E_{not-false}(A) = 0
   ```

3. **单元传播推导**：
   ```
   E(D) = max[min(Σ_{i∈[c]} E(C_i)𝟙_{E(C_i)·E_{not-false}(A)=1}, 1) - E_{assigned}(A), 0]
   ```

#### 引理4.8：并行子句处理
给定编码矩阵X_encoding，存在：
- 输出𝟙_{A⊨F}的注意力头（误差≤ε）
- 输出𝟙_{F⊨¬A}的注意力头（误差≤ε）  
- 输出E(D)的注意力头+MLP层（误差≤ε）

## 2. 模型架构和设计

### 2.1 Transformer架构配置

#### 基础架构（70M参数版本）
```python
model_config = {
    "n_layer": 6,           # 层数
    "n_head": 8,            # 注意力头数
    "n_embd": 512,          # 嵌入维度
    "n_head_dim": 512,      # 注意力隐藏维度
    "n_mlp_dim": 2048,      # MLP隐藏维度
    "block_size": 850,      # 上下文长度
    "vocab_size": ~50,      # 词汇表大小
    "model": "llama"        # 架构类型
}
```

#### 大规模版本（160M参数）
```python
large_config = {
    "n_layer": 12,
    "n_head": 12,
    "n_embd": 768,
    "n_head_dim": 768,
    "n_mlp_dim": 3072,
    "block_size": 850
}
```

### 2.2 自定义组件

#### SAT专用分词器
```python
class SATTokenizer:
    - 处理DIMACS格式输入
    - 支持变量标记：'1', '2', ..., 'n', '-1', '-2', ..., '-n'
    - 特殊标记：'0'(分隔符), '[SEP]', '[BT]', 'D', 'SAT', 'UNSAT'
    - 编码/解码CNF公式和推理轨迹
```

#### 自定义数据集类
```python
class SATDataset(Dataset):
    - 加载DIMACS格式的SAT问题
    - 生成对应的求解轨迹
    - 支持动态批处理
    - 处理变量置换增强
```

### 2.3 PARAT工具实现

#### 核心抽象
PARAT将Transformer权重构造抽象为高级数组操作：

```python
# 示例：查找最近的分隔符token
def nearest_token(tok_emb, vocab, targets, indices):
    target_tok_ids = [vocab.index(t) for t in targets]
    target_tokens = Concat([tok_emb[:, id] for id in target_tok_ids])
    in_targets = Linear(target_tokens, np.ones((1, len(targets))))
    filtered_index = indices * in_targets
    return Mean(ones, filtered_index, target_tokens)
```

#### 编译流程
1. **计算图构建**：构建操作依赖树
2. **降级到基础操作**：SelfAttention、GLUMLP、Linear等
3. **层分配**：拓扑排序确定层次结构
4. **权重赋值**：生成PyTorch模型参数

## 3. 关键技术实现

### 3.1 思维链(CoT)格式

#### 输入格式（DIMACS）
```
[BOS] -2 -4 -1 0 3 4 -1 0 -1 -3 -2 0 1 -2 -4 0 [SEP]
```

#### CoT输出格式
```
D 2 D 1 -4 3 [BT] D 2 -1 -4 [BT] -2 D 3 D 4 1 SAT
```

标记说明：
- `D`：决策文字标记
- `[BT]`：回溯操作
- 数字：变量赋值
- `SAT/UNSAT`：最终结果

### 3.2 七层Transformer的功能分解

**Layer 1**：准备查找分隔符和D标记
- 计算i_sep、i_D
- 计算(i-1)²、i²用于索引比较

**Layer 2**：执行COPY操作
- 复制最后分隔符的索引和类型
- 计算最近D标记位置
- 计算(p_sep')²

**Layer 3**：复制前一个token的值
- 获取前一个分隔符位置
- 判断当前是否为决策变量
- 计算偏移量d_sep

**Layer 4**：核心逻辑运算
- 计算E(B_i)编码
- 执行回溯相关计算
- 比较操作准备

**Layer 5**：执行引理4.7的逻辑运算
- 计算𝟙_{A⊨F}、𝟙_{F⊨¬A}、E(D)
- 布尔运算组合

**Layer 6**：最终布尔运算
- 计算b_unsat、b_[BT]等
- 准备输出门控

**Layer 7**：输出层
- 根据优先级输出相应token

### 3.3 注意力机制的数学实现

#### 饱和注意力(Saturated Attention)
```python
A = XW_Q(W_K·X)^T
M_i = {j ∈ [i] | A_{ij} = max_k A_{ik}}
SaturatedAttn(X)_i = Σ_{j∈M_i} X_j·W_V / |M_i|
```

#### Softmax近似
通过缩放因子β控制近似精度：
```python
Attn(X) = softmax(β·XW_Q(W_K·X)^T/√d_h + M)·XW_V
```
误差界：||o_i - o_i'||_∞ ≤ ε，其中ε随β指数级减小

## 4. 数学公式和理论基础

### 4.1 编码定义（定义4.6）

对于集合B ∈ B，定义映射E、E_{not-false}、E_{assigned}：

```
E(B)_v = 𝟙_{x_v∈B}
E(B)_{v+p} = 𝟙_{¬x_v∈B}

E_{not-false}(B)_v = 𝟙_{¬x_v∉B}
E_{not-false}(B)_{v+p} = 𝟙_{x_v∉B}

E_{assigned}(B)_v = E_{assigned}(B)_{v+p} = 𝟙_{x_v∈B∨¬x_v∈B}
```

### 4.2 MLP与ReGLU激活

ReGLU激活函数：
```
σ_{ReGLU}(u_i) = u_{i,1} ⊗ ReLU(u_{i,2})
```

能够模拟：
- 2层ReLU MLP
- 线性运算
- 逐元素乘法

### 4.3 复杂度分析

- **时间复杂度**：每个前向传播O(n·d²)，其中n是序列长度
- **空间复杂度**：O(p²)参数存储
- **CoT步骤上界**：最坏情况p·2^(p+1)，实践中通常为8p·2^(0.08p)

## 5. 代码结构和关键模块

### 5.1 项目目录结构
```
src/
├── core/               # 核心训练和评估
│   ├── train.py       # 主训练脚本
│   ├── trainer.py     # 自定义HuggingFace trainer
│   └── evaluator.py   # 模型评估
├── dataset/           # 数据处理
│   └── sat_dataset.py # SAT数据集和分词器
├── models/            
│   ├── parat/         # PARAT工具实现
│   │   ├── model.py   # 模型定义
│   │   ├── compiler.py # 编译器
│   │   └── sops.py    # 符号操作
│   └── sat_solver/    # SAT求解器
│       ├── dpll.py    # DPLL实现
│       ├── cdcl.py    # CDCL实现
│       └── generate_formula.py # 公式生成
└── utils/             
    ├── configurator.py # 配置系统
    └── utils.py       # 通用工具
```

### 5.2 关键类和函数

#### 训练器扩展
```python
class SATTrainer(Trainer):
    - compute_loss(): 自定义损失计算
    - evaluation_loop(): SAT特定评估
    - log_metrics(): 记录SAT/UNSAT准确率
```

#### 评估器
```python
class SATEvaluator:
    - evaluate_accuracy(): 计算SAT/UNSAT准确率
    - evaluate_trace(): 验证推理轨迹正确性
    - analyze_errors(): 错误分析
```

### 5.3 配置系统

采用nanoGPT风格的Python配置文件：
```python
# configs/train_sat_6_10_random.py
config = {
    "dataset": "data/datasets/SAT_6_10_Random_State_Large",
    "out_dir": "results/models/sat_6_10_random",
    "block_size": 512,
    "batch_size": 64,
    "learning_rate": 6e-4,
    "n_layer": 6,
    "n_head": 8,
    "n_embd": 512,
    "dropout": 0.1,
    "epochs": 5,
    "model": "llama",
    "state_trace": True
}
```

## 6. 实现优化

### 6.1 内存优化
- 梯度累积减少批次内存需求
- 混合精度训练(fp16/bf16)
- 梯度检查点技术

### 6.2 训练优化
- 学习率预热和余弦退火
- AdamW优化器配置
- 变量ID置换数据增强

### 6.3 推理优化
- KV缓存避免重复计算
- 批量推理提高吞吐量
- 早停机制减少不必要的生成